services:
  # ============================================================================
  # OpenWebUI - ChatGPT-like Web Interface
  # ============================================================================
  # Provides the user interface for interacting with LLMs
  # Access at: http://localhost:3000
  # ============================================================================
  open-webui:
    build:
      context: .
      dockerfile: Dockerfile.openwebui
    container_name: open-webui
    ports:
      - "3000:8080"  # Host:Container
    environment:
      # ===== LLM Backend Configuration =====
      # Points to our proxy that translates to LM Studio format
      - OPENAI_API_BASE_URL=http://lm-studio-proxy:5003/v1
      - OPENAI_API_KEY=sk-placeholder  # Dummy key (not validated)

      # ===== Branding Configuration =====
      # Customize the application name and appearance
      - WEBUI_NAME=LinoGPT
      - WEBUI_URL=http://localhost:3000
      - RESPONSE_WATERMARK=Generated by LinoGPT

      # ===== Authentication & User Management =====
      # WEBUI_AUTH: Enable/disable login requirement
      # ENABLE_SIGNUP: Allow new user registration
      # DEFAULT_USER_ROLE: Role for new users (user/admin/pending)
      - WEBUI_AUTH=true
      - ENABLE_SIGNUP=true
      - DEFAULT_USER_ROLE=user

      # ===== Logging =====
      - WEBUI_LOG_LEVEL=INFO

      # ===== Optional: Other Configurations =====
      # Uncomment to use:
      # - DEFAULT_MODELS=your-model-name  # Pre-select a model
      # - WEBUI_AUTH=false  # Disable auth for family use (less secure)

    volumes:
      - open-webui-data:/app/backend/data  # Persistent storage for chats/users
    restart: unless-stopped
    networks:
      - openwebui-network
    depends_on:
      - lm-studio-proxy  # Ensure proxy starts first

  # ============================================================================
  # LM Studio MCP Proxy - OpenAI â†’ LM Studio API Translator
  # ============================================================================
  # Converts OpenAI API requests to LM Studio's /v1/responses format
  # Auto-discovers MCP servers from LM Studio's mcp.json file
  # Handles streaming, thinking tokens, and tool execution indicators
  # ============================================================================
  lm-studio-proxy:
    build:
      context: .
      dockerfile: Dockerfile.proxy
    container_name: lm-studio-proxy
    ports:
      - "5003:5003"  # Proxy API endpoint
    volumes:
      # ===== IMPORTANT: Update this path =====
      # Mount your LM Studio mcp.json file (read-only)
      # macOS/Linux: ~/.lmstudio/mcp.json
      # Windows: %USERPROFILE%\.lmstudio\mcp.json
      - /Users/linovaldovinos/.lmstudio/mcp.json:/app/mcp.json:ro
      # ========================================
    restart: unless-stopped
    networks:
      - openwebui-network
    # Note: LM_STUDIO_BASE URL is configured in lm-studio-proxy.py line 22
    # Update it to your host machine's IP address (e.g., http://192.168.1.100:5002)

volumes:
  open-webui-data:
    driver: local

networks:
  openwebui-network:
    driver: bridge
